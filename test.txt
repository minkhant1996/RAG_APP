Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: The atis-3 corpus. In Proceedings of the Workshop on Human Language Technology, pages 43—48. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.
Zelle and Raymond J. Mooney.
Patti Price.
1996. Learn- ing to parse database queries using inductive logic programming. In Proceedings of the Thirteenth Na- tional Conference on Artificial Intelligence - Volume 2, pages 1050-1055. A Appendix



A.1 Prompt Examples

Below contains an example of a zero-shot nor- malized prompt, which contains the database Network_1 from Spider (Yu et al., 2018), a task instruction “Using valid SQLite, answer the fol- lowing questions for the tables provided above.”, and a test question “How many high schoolers are there?”. Zero-shot normalized

prompt create table highschooler ( id int primary key, name text, grade int 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /* 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) 3 example rows: select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite, answer the questions for the tables provided Question: How many high schoolers select
following above. are there? Below contains an example of a 4-shot single- domain normalized prompt, which contains a database prompt and 4 demonstration examples ahead of the test question. 4-shot single-domain normalized prompt create table highschooler ( id int primary key, name text, grade int )5 /% 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 x/ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /% 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 x/ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) )5 /% 3 example rows: select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 x/ -- Using valid SQLite, answer the questions for the tables provided above. Question: What is Kyle's id? select id from highschooler where name Kyle'; Question: Return the names of friends high school student Kyle. select t3.name from friend as t1 join highschooler as t2 on t1.student_id = join highschooler as t3 on t1.friend_id .id where t2.name = 'Kyle'; Question: Show names of all high school students who do not have any friends. select name from highschooler except t2.name from friend as t1 join highschooler as t2 on tl.student_id = t2.id;
following
=


of the



t2.id

=


t3



select

Question: What are the names and grades for each high schooler? select name, grade from highschooler; Question: How many high schoolers are there? select
Below contains an example of a 4-shot cross- domain prompt, which contains 2 demonstration databases, each with 2 demonstration examples ahead of the test database and question. cross-domain prompt create table publication ( publication_id int, book_id int, publisher text, publication_date text, price real, primary key (publication_id), foreign key (book_id) references book( book_id) )5 /* 3 example rows: select * from publication limit 3; publication_id book_id publisher publication_date price 1 1 Pearson August 2008 15000000.0 2 3 Thomson Reuters March 2008 6000000.0 3 4 Wiley June 2006 4100000.0 */ create table book ( book_id int, title text, issues real, writer text, primary key (book_id) )5 /* 3 example rows: select x from book limit 3; book_id title issues writer 1 The Black Lamb 6.0 Timothy Truman 2 Bloody Mary 4.0 Garth Ennis 3 Bloody Mary : Lady Liberty 4.0 Garth Ennis */ -- Using valid SQLite, answer the following questions for the tables provided above. Question: List the writers of the books in ascending alphabetical order. select writer from book order by writer asc; Question: How many books are there? select count(*) from book; create table race ( race_id int, name text, class text, date text, track_id text, primary key (race_id), foreign key (track_id) references track( track_id) )5 /* 3 example rows: select x from race limit 3; race_id name class date track_id 1 Rolex 24 At Daytona DP/GT January 26 January 27 1 2 Gainsco Grand Prix of Miami DP/GT March 29 2
3


Mexico City 250 2



DP/GT



April 19

/
create table track ( track_id int, name text, location text,
seating real, year_opened real, primary key (track_id) )5 /% 3 example rows: select * from track limit 3; track_id name location seating year_opened 1 Auto Club Speedway Fontana, CA 92000.0 1997.0 2 Chicagoland Speedway Joliet, IL 75000.0 2001.0 3 Darlington Raceway Darlington, SC 63000.0 1950.0 x/ -- Using valid SQLite, answer the following questions for the tables provided above. Question: Show the name and location for tracks . select name, location from the track; Question: Show the name of track and the number of races in each track. select t2.name, count(*) from race as t1 join track as t2 on tl.track_id = t2. track_id group by t1.track_id; create table highschooler ( id int primary key, name text, grade int )5 /% 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 x/ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /% 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 x/ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) )5 /%
3 example rows: select * from likes limit 3; student_id liked_id 1689 1709
1709 1782 x/
1689 1709
-- Using valid SQLite, answer the following questions for the tables provided above. Question: How many high schoolers are there? select


all



A.2 Tests of Significance

Table 1 contains the performance of Codex and ChatGPT using different database prompt construc- tions in the zero-shot setting. We observe that the normalization results in slightly improved perfor- mance for all database prompt constructions with Codex and 6 out of 7 database prompt construc- tions with ChatGPT. It is important to note, how- ever, that when comparing normalized and unnor- malized database prompt constructions using the same method, the results did not demonstrate statis- tical significance in McNemar’s test, with p-values greater than 0.05. Nevertheless, the primary advan- tage of normalization lies in its ability to reduce variations among different databases and minimize the overall prompt length. When evaluating various prompt structions, we note the advantages gained from incorporating both table relationships (Columns=[]+ForeignKey vs Columns=[]) and table content (CreateTable+SelectCol 3 vs CreateTable) are mostly statistically significant in McNemar'’s test, with p-values smaller than 0.05. Table 3 displays the results of the significant tests. The performance of Columns=[]+ForeignKey compared to Columns=[] is statistically sig- nificant in all cases, except for codex with normalized prompts. Likewise, the performance of CreateTable+SelectCol 3 is statistically significant for both Codex and ChatGPT, with both normalized and unnormalized prompts, when com- pared to CreateTable. These significant findings highlight the effectiveness of incorporating table relationships and database content. con-


A.3 Detailed Single-domain Results

Tables 4 and 5 provide detailed results of Codex and ChatGPT in the single-domain setting, respec- tively. The performance of both models is also illustrated in Figure 5. A.4 TImpact of Demonstration Prompt for ChatGPT-16K

Figure 8 presents the accuracy of ChatGPT-16K corresponding to different combinations of the number of databases and the number of examples per database used as demonstrations. Similar to our findings with Codex, presenting more databases does not always lead to improved performance for ChatGPT-16K. For a fixed number of examples per database, we observe an initial increase in its
###IMAGE###Figure-8###IMAGE###
Figure 8: A heat map of ChatGPT-16K’s execution ac- curacy using CreateTable+SelectRow 3 for different numbers of databases and examples per database in the demonstration. Darker color indicates higher accuracy. ###IMAGE###Figure-8###IMAGE###
Figure 9: Execution accuracy of ChatGPT-16K in re- lation to the length of prompts. Each dot represents a demonstration construction, with the m, k denoting the number of databases and examples per database. The lines represent second-degree polynomial trendlines fit- ted to the results. performance as the number of databases increases, however, this improvement is followed by a de- crease once the number of databases reaches a cer- tain threshold. To understand this phenomenon, we analyze the results in relation to the prompt length. Figure 9 shows the relationship between the ac- curacy of different demonstration prompts and their prompt lengths. Similar to Codex, the performance of ChatGPT-16K also exhibits an inverted-U shape as the prompt length increases for each number of examples per database. Additionally, we observe the performance starts to decrease once the prompt text length exceeds approximately 11K tokens. While Codex supports 8K tokens and ChatGPT- 16K supports 16K tokens, we notice that their performance tends to decline when dealing with demonstrations that exceed approximately 70% of the maximum prompt length. <table><thead><tr><th>Prompt 1</th><th>Prompt 2</th><th>LLM</th><th>Normalization</th><th>Significant Test</th></tr></thead><tbody><tr><td rowspan="4">Columns=[]</td><td rowspan="4">Columns=[J+ForeignKey</td><td>Codex</td><td>U</td><td>v</td></tr><tr><td>Codex</td><td></td><td>%</td></tr><tr><td>ChatGPT</td><td></td><td>N</td></tr><tr><td>ChatGPT</td><td>Zlc|Z|c|z|c|z</td><td>SIS N</td></tr><tr><td rowspan="4">CreateTable</td><td rowspan="4">CreateTable+SelectCol 3</td><td>Codex</td><td></td><td></td></tr><tr><td>Codex</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td></td></tr></tbody></table>

Table 3: Tests of Statistical Significance for comparing different prompt constructions. Prompt 1 and Prompt 2 were used to represent two distinct methods of constructing prompts in McNemar’s test. The prompts were categorized as U and N, representing unnormalized and normalized database prompts, respectively. The v symbol indicates that the p-value is smaller than 0.05, indicating statistical significance, while the X symbol indicates p-values greater than 0.05, indicating a lack of statistical significance. <table><thead><tr><th colspan="2">Database Prompt Construction</th><th></th><th rowspan="2">0-shot 719</th><th rowspan="2">1-shot 70.7</th><th rowspan="2">4-shot 74.9</th><th rowspan="2">8-shot 78.0</th><th rowspan="2">16-shot 81.6</th></tr></thead><tbody><tr><td rowspan="2">Table Schema</td><td>Table(Columns)</td><td></td></tr><tr><td>Columns=[]</td><td></td><td>718</td><td>721</td><td>755</td><td>780</td><td>81.6</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>73.1</td><td>732</td><td>761</td><td>78.5</td><td>81.6</td></tr><tr><td>CreateTable</td><td></td><td>73.1</td><td>724</td><td>760</td><td>787</td><td>81.3</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.9</td><td>72.9</td><td>71.6</td><td>80.5</td><td>82.5</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>74.1</td><td>73.4</td><td>713</td><td>80.5</td><td>82.9</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>75.7</td><td>74.1</td><td>71.9</td><td>80.7</td><td>82.5</td></tr></tbody></table>

Table 4: Single-domain results of Codex using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined. <table><thead><tr><th colspan="2">Database Prompt Construction</th><th></th><th rowspan="2">0-shot 70.5</th><th rowspan="2">1-shot 71.6</th><th rowspan="2">4-shot 74.3</th><th rowspan="2">8-shot 774</th><th rowspan="2">16-shot 79.4</th></tr><tr><th>Table Schema</th><th>Table(Columns)</th><th></th></tr></thead><tbody><tr><td></td><td>Columns=[]</td><td></td><td>69.1</td><td>70.7</td><td>74.4</td><td>77.8</td><td>79.5</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>71.2</td><td>734</td><td>75.4</td><td>78.4</td><td>80.0</td></tr><tr><td>CreateTable</td><td></td><td>71.7</td><td>73.1</td><td>75.8</td><td>78.0</td><td>79.5</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.8</td><td>72.8</td><td>76.6</td><td>79.1</td><td>81.6</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>72.1</td><td>73.3</td><td>76.4</td><td>78.9</td><td>81.3</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>73.6</td><td>73.8</td><td>76.8</td><td>79.8</td><td>814</td></tr></tbody></table>

Table 5: Single-domain results of ChatGPT using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined.

(min_rag) minkhant@MinKhant:~/Documents/MyFolder/freelance/RAG_APP/src$ python3 test.py 
{'userId': 'user123', 'pdf': [{'pdf_filename': 'pdf1', 'pdf_title': None, 'image_path': []}], 'chat_history': [{'query': 'What is the capital of India?', 'response': None, 'timestamp': {'query': '2024-11-17 15:11:10', 'response': None}, 'input_tokens': None, 'output_tokens': None}]}
{'userId': 'user123', 'pdf': [{'pdf_filename': 'pdf1', 'pdf_title': None, 'image_path': []}], 'chat_history': [{'query': 'What is the capital of India?', 'response': 'New response', 'timestamp': {'query': '2024-11-17 15:11:10', 'response': '2024-11-17 15:11:10'}, 'input_tokens': None, 'output_tokens': None}]}
(2021); Poesia et al. (2022) have demonstrated the effectiveness of similarity-based demonstration retrieval in the cross-domain setting. Additionally, Levy et al. (2022) have highlighted the advantages of incor- porating diverse demonstrations for compositional generalization. Furthermore, Pourreza and Rafiei (2023) and Chen et al. (2023) incorporate interme- diate steps in prompts and unlock LLMs’ capability of self-correcting their predictions. conducting a comprehensive evaluation of prompt representations across different text-to-SQL set- tings. While there are similar motivations to the work by Rajkumar et al. (2022), which analyzes the performance of CodeX on Spider for the zero-shot setting and on two databases for the single-domain setting, we aim to provide more general findings by evaluating across a wider range of databases and considering all three text-to-SQL settings. Table Representation Encoding structured databases with neural models has been a persistent challenge. To encode database schema, graph neural networks are utilized to represent the rela- tionships among tables (Bogin et al., 2019; Chen et al., 2021b). Alternatively, other studies (Guo et al., 2019; Lin et al., 2020; Shaw et al., 2020) have converted table schemas into a sequence to effectively leverage pretrained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020). In such cases, table relationships can be encoded as meta-data features (Lin et al., 2020) or used as a guide for attention mechanism (Wang et al., 2019; Cao et al., 2021; Li et al., 2023b). To incorporate table content into neural mod- els, prior supervised methods provide question- specific table content by identifying the relevant table content mentioned in the question through string matching (Lin et al., 2020; Shaw et al., 2020). However, Chang et al. (2023) have revealed the vulnerability of string matching to perturbations. Given that LLMs with in-context learning support longer input sequences compared to supervised methods, we follow previous work to provide table content without explicitly considering the questions (Rajkumar et al., 2022; Chen et al., 2023). In contrast to these approaches, our focus lies in


7 Conclusions

In this paper, we investigate effective prompt- ing strategies in the text-to-SQL task. We thor- oughly compare various prompt construction strate- gies for databases and demonstrations in the zero- shot, single-domain, and cross-domain text-to-SQL. Through our investigation, we uncover the critical database knowledge and optimal representations for effective prompting. Additionally, an interest- ing finding is the existence of a sweet spot in terms of prompt length for Codex in the cross-domain setting. Overall, we believe that our findings will provide valuable guidance for future research in the field of text-to-SQL with LLMs. Limitation

We conducted our experiments using 20 databases from the Spider dataset, with the goal of providing general findings for text-to-SQL prompt construc- tions. However, our findings may not always be applicable to a specific database, particularly if the database is significantly different from the Spider databases. For the single-domain and cross-domain text-to-SQL scenarios, we conduct our experiments multiple times, each involving randomly selecting demonstrations with different random seeds, how- ever, we did not investigate the effectiveness of prompt constructions with different demonstration- retrieval strategies or intermediate reasoning steps. Ethics Statement

We acknowledge the importance of the ACL Ethics Policy and agree with it. In this paper, we use Ope- nAI Codex and ChatGPT as our language models 4. Codex is currently free for research purposes, the cost of ChatGPT is around $200. The code for the paper is included in the supplementary ma- terials and will be publicly released to facilitate reproducibility. “API is available at https://openai.com/api/. References

Ben Bogin, Matt Gardner, and Jonathan Berant.
Patti Price.

John M.
Torsten Scholak, Nathan Schucher, and Dzmitry Bah- danau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9895-9901. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2020. Compositional general- ization and natural language variation: Can a seman- tic parsing approach handle both? arXiv preprint arXiv:2010.12725.

(min_rag) minkhant@MinKhant:~/Documents/MyFolder/freelance/RAG_APP/src$ python3 test.py 
{'userId': 'user123', 'pdf': [{'pdf_filename': 'pdf1', 'pdf_title': None, 'image_path': []}], 'chat_history': [{'query': 'What is the capital of India?', 'response': None, 'timestamp': {'query': '2024-11-17 15:21:54', 'response': None}, 'input_tokens': None, 'output_tokens': None}]}
{'userId': 'user123', 'pdf': [{'pdf_filename': 'pdf1', 'pdf_title': None, 'image_path': []}], 'chat_history': [{'query': 'What is the capital of India?', 'response': 'New response', 'timestamp': {'query': '2024-11-17 15:21:54', 'response': '2024-11-17 15:21:54'}, 'input_tokens': None, 'output_tokens': None}]}
[Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574446+07:00'}, page_content='(2021); Poesia et al. (2022) have demonstrated the effectiveness of similarity-based demonstration retrieval in the cross-domain setting. Additionally, Levy et al. (2022) have highlighted the advantages of incor- porating diverse demonstrations for compositional generalization. Furthermore, Pourreza and Rafiei (2023) and Chen et al. (2023) incorporate interme- diate steps in prompts and unlock LLMs’ capability of self-correcting their predictions. conducting a comprehensive evaluation of prompt representations across different text-to-SQL set- tings. While there are similar motivations to the work by Rajkumar et al. (2022), which analyzes the performance of CodeX on Spider for the zero-shot setting and on two databases for the single-domain setting, we aim to provide more general findings by evaluating across a wider range of databases and considering all three text-to-SQL settings. Table Representation Encoding structured databases with neural models has been a persistent challenge. To encode database schema, graph neural networks are utilized to represent the rela- tionships among tables (Bogin et al., 2019; Chen et al., 2021b). Alternatively, other studies (Guo et al., 2019; Lin et al., 2020; Shaw et al., 2020) have converted table schemas into a sequence to effectively leverage pretrained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020). In such cases, table relationships can be encoded as meta-data features (Lin et al., 2020) or used as a guide for attention mechanism (Wang et al., 2019; Cao et al., 2021; Li et al., 2023b). To incorporate table content into neural mod- els, prior supervised methods provide question- specific table content by identifying the relevant table content mentioned in the question through string matching (Lin et al., 2020; Shaw et al., 2020). However, Chang et al. (2023) have revealed the vulnerability of string matching to perturbations. Given that LLMs with in-context learning support longer input sequences compared to supervised methods, we follow previous work to provide table content without explicitly considering the questions (Rajkumar et al., 2022; Chen et al., 2023). In contrast to these approaches, our focus lies in\n\n\n7 Conclusions\n\nIn this paper, we investigate effective prompt- ing strategies in the text-to-SQL task. We thor- oughly compare various prompt construction strate- gies for databases and demonstrations in the zero- shot, single-domain, and cross-domain text-to-SQL. Through our investigation, we uncover the critical database knowledge and optimal representations for effective prompting. Additionally, an interest- ing finding is the existence of a sweet spot in terms of prompt length for Codex in the cross-domain setting. Overall, we believe that our findings will provide valuable guidance for future research in the field of text-to-SQL with LLMs. Limitation\n\nWe conducted our experiments using 20 databases from the Spider dataset, with the goal of providing general findings for text-to-SQL prompt construc- tions. However, our findings may not always be applicable to a specific database, particularly if the database is significantly different from the Spider databases. For the single-domain and cross-domain text-to-SQL scenarios, we conduct our experiments multiple times, each involving randomly selecting demonstrations with different random seeds, how- ever, we did not investigate the effectiveness of prompt constructions with different demonstration- retrieval strategies or intermediate reasoning steps. Ethics Statement\n\nWe acknowledge the importance of the ACL Ethics Policy and agree with it. In this paper, we use Ope- nAI Codex and ChatGPT as our language models 4. Codex is currently free for research purposes, the cost of ChatGPT is around $200. The code for the paper is included in the supplementary ma- terials and will be publicly released to facilitate reproducibility. “API is available at https://openai.com/api/. References\n\nBen Bogin, Matt Gardner, and Jonathan Berant.'), Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574563+07:00'}, page_content='1996. Learn- ing to parse database queries using inductive logic programming. In Proceedings of the Thirteenth Na- tional Conference on Artificial Intelligence - Volume 2, pages 1050-1055. A Appendix\n\n\n\nA.1 Prompt Examples\n\nBelow contains an example of a zero-shot nor- malized prompt, which contains the database Network_1 from Spider (Yu et al., 2018), a task instruction “Using valid SQLite, answer the fol- lowing questions for the tables provided above.”, and a test question “How many high schoolers are there?”. Zero-shot normalized\n\nprompt create table highschooler ( id int primary key, name text, grade int 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /* 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) 3 example rows: select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite, answer the questions for the tables provided Question: How many high schoolers select\nfollowing above. are there? Below contains an example of a 4-shot single- domain normalized prompt, which contains a database prompt and 4 demonstration examples ahead of the test question. 4-shot single-domain normalized prompt create table highschooler ( id int primary key, name text, grade int )5 /% 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 x/ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /% 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 x/ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) )5 /% 3 example rows: select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 x/ -- Using valid SQLite, answer the questions for the tables provided above. Question: What is Kyle\'s id? select id from highschooler where name Kyle\'; Question: Return the names of friends high school student Kyle. select t3.name from friend as t1 join highschooler as t2 on t1.student_id = join highschooler as t3 on t1.friend_id .id where t2.name = \'Kyle\'; Question: Show names of all high school students who do not have any friends. select name from highschooler except t2.name from friend as t1 join highschooler as t2 on tl.student_id = t2.id;\nfollowing\n=\n\n\nof the\n\n\n\nt2.id\n\n=\n\n\nt3\n\n\n\nselect\n\nQuestion: What are the names and grades for each high schooler? select name, grade from highschooler; Question: How many high schoolers are there? select\nBelow contains an example of a 4-shot cross- domain prompt, which contains 2 demonstration databases, each with 2 demonstration examples ahead of the test database and question. cross-domain prompt create table publication ( publication_id int, book_id int, publisher text, publication_date text, price real, primary key (publication_id), foreign key (book_id) references book( book_id) )5 /* 3 example rows: select * from publication limit 3; publication_id book_id publisher publication_date price 1 1 Pearson August 2008 15000000.0 2 3 Thomson Reuters March 2008 6000000.0 3 4 Wiley June 2006 4100000.0 */ create table book ( book_id int, title text, issues real, writer text, primary key (book_id) )5 /* 3 example rows: select x from book limit 3; book_id title issues writer 1 The Black Lamb 6.0 Timothy Truman 2 Bloody Mary 4.0 Garth Ennis 3 Bloody Mary : Lady Liberty 4.0 Garth Ennis */ -- Using valid SQLite, answer the following questions for the tables provided above. Question: List the writers of the books in ascending alphabetical order. select writer from book order by writer asc; Question: How many books are there? select count(*) from book; create table race ( race_id int, name text, class text, date text, track_id text, primary key (race_id), foreign key (track_id) references track( track_id) )5 /* 3 example rows: select x from race limit 3; race_id name class date track_id 1 Rolex 24 At Daytona DP/GT January 26 January 27 1 2 Gainsco Grand Prix of Miami DP/GT March 29 2\n3\n\n\nMexico City 250 2\n\n\n\nDP/GT\n\n\n\nApril 19\n\n/\ncreate table track ( track_id int, name text, location text,\nseating real, year_opened real, primary key (track_id) )5 /% 3 example rows: select * from track limit 3; track_id name location seating year_opened 1 Auto Club Speedway Fontana, CA 92000.0 1997.0 2 Chicagoland Speedway Joliet, IL 75000.0 2001.0 3 Darlington Raceway Darlington, SC 63000.0 1950.0 x/ -- Using valid SQLite, answer the following questions for the tables provided above. Question: Show the name and location for tracks . select name, location from the track; Question: Show the name of track and the number of races in each track. select t2.name, count(*) from race as t1 join track as t2 on tl.track_id = t2. track_id group by t1.track_id; create table highschooler ( id int primary key, name text, grade int )5 /% 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 x/ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /% 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 x/ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) )5 /%\n3 example rows: select * from likes limit 3; student_id liked_id 1689 1709\n1709 1782 x/\n1689 1709\n-- Using valid SQLite, answer the following questions for the tables provided above. Question: How many high schoolers are there? select\n\n\nall\n\n\n\nA.2 Tests of Significance\n\nTable 1 contains the performance of Codex and ChatGPT using different database prompt construc- tions in the zero-shot setting. We observe that the normalization results in slightly improved perfor- mance for all database prompt constructions with Codex and 6 out of 7 database prompt construc- tions with ChatGPT. It is important to note, how- ever, that when comparing normalized and unnor- malized database prompt constructions using the same method, the results did not demonstrate statis- tical significance in McNemar’s test, with p-values greater than 0.05. Nevertheless, the primary advan- tage of normalization lies in its ability to reduce variations among different databases and minimize the overall prompt length. When evaluating various prompt structions, we note the advantages gained from incorporating both table relationships (Columns=[]+ForeignKey vs Columns=[]) and table content (CreateTable+SelectCol 3 vs CreateTable) are mostly statistically significant in McNemar\'’s test, with p-values smaller than 0.05. Table 3 displays the results of the significant tests. The performance of Columns=[]+ForeignKey compared to Columns=[] is statistically sig- nificant in all cases, except for codex with normalized prompts. Likewise, the performance of CreateTable+SelectCol 3 is statistically significant for both Codex and ChatGPT, with both normalized and unnormalized prompts, when com- pared to CreateTable. These significant findings highlight the effectiveness of incorporating table relationships and database content. con-\n\n\nA.3 Detailed Single-domain Results\n\nTables 4 and 5 provide detailed results of Codex and ChatGPT in the single-domain setting, respec- tively. The performance of both models is also illustrated in Figure 5. A.4 TImpact of Demonstration Prompt for ChatGPT-16K\n\nFigure 8 presents the accuracy of ChatGPT-16K corresponding to different combinations of the number of databases and the number of examples per database used as demonstrations. Similar to our findings with Codex, presenting more databases does not always lead to improved performance for ChatGPT-16K. For a fixed number of examples per database, we observe an initial increase in its\n###IMAGE###Figure-8###IMAGE###\nFigure 8: A heat map of ChatGPT-16K’s execution ac- curacy using CreateTable+SelectRow 3 for different numbers of databases and examples per database in the demonstration. Darker color indicates higher accuracy. ###IMAGE###Figure-8###IMAGE###\nFigure 9: Execution accuracy of ChatGPT-16K in re- lation to the length of prompts. Each dot represents a demonstration construction, with the m, k denoting the number of databases and examples per database. The lines represent second-degree polynomial trendlines fit- ted to the results. performance as the number of databases increases, however, this improvement is followed by a de- crease once the number of databases reaches a cer- tain threshold. To understand this phenomenon, we analyze the results in relation to the prompt length. Figure 9 shows the relationship between the ac- curacy of different demonstration prompts and their prompt lengths. Similar to Codex, the performance of ChatGPT-16K also exhibits an inverted-U shape as the prompt length increases for each number of examples per database. Additionally, we observe the performance starts to decrease once the prompt text length exceeds approximately 11K tokens. While Codex supports 8K tokens and ChatGPT- 16K supports 16K tokens, we notice that their performance tends to decline when dealing with demonstrations that exceed approximately 70% of the maximum prompt length. <table><thead><tr><th>Prompt 1</th><th>Prompt 2</th><th>LLM</th><th>Normalization</th><th>Significant Test</th></tr></thead><tbody><tr><td rowspan="4">Columns=[]</td><td rowspan="4">Columns=[J+ForeignKey</td><td>Codex</td><td>U</td><td>v</td></tr><tr><td>Codex</td><td></td><td>%</td></tr><tr><td>ChatGPT</td><td></td><td>N</td></tr><tr><td>ChatGPT</td><td>Zlc|Z|c|z|c|z</td><td>SIS N</td></tr><tr><td rowspan="4">CreateTable</td><td rowspan="4">CreateTable+SelectCol 3</td><td>Codex</td><td></td><td></td></tr><tr><td>Codex</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td></td></tr></tbody></table>\n\nTable 3: Tests of Statistical Significance for comparing different prompt constructions. Prompt 1 and Prompt 2 were used to represent two distinct methods of constructing prompts in McNemar’s test. The prompts were categorized as U and N, representing unnormalized and normalized database prompts, respectively. The v symbol indicates that the p-value is smaller than 0.05, indicating statistical significance, while the X symbol indicates p-values greater than 0.05, indicating a lack of statistical significance. <table><thead><tr><th colspan="2">Database Prompt Construction</th><th></th><th rowspan="2">0-shot 719</th><th rowspan="2">1-shot 70.7</th><th rowspan="2">4-shot 74.9</th><th rowspan="2">8-shot 78.0</th><th rowspan="2">16-shot 81.6</th></tr></thead><tbody><tr><td rowspan="2">Table Schema</td><td>Table(Columns)</td><td></td></tr><tr><td>Columns=[]</td><td></td><td>718</td><td>721</td><td>755</td><td>780</td><td>81.6</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>73.1</td><td>732</td><td>761</td><td>78.5</td><td>81.6</td></tr><tr><td>CreateTable</td><td></td><td>73.1</td><td>724</td><td>760</td><td>787</td><td>81.3</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.9</td><td>72.9</td><td>71.6</td><td>80.5</td><td>82.5</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>74.1</td><td>73.4</td><td>713</td><td>80.5</td><td>82.9</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>75.7</td><td>74.1</td><td>71.9</td><td>80.7</td><td>82.5</td></tr></tbody></table>\n\nTable 4: Single-domain results of Codex using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined. <table><thead><tr><th colspan="2">Database Prompt Construction</th><th></th><th rowspan="2">0-shot 70.5</th><th rowspan="2">1-shot 71.6</th><th rowspan="2">4-shot 74.3</th><th rowspan="2">8-shot 774</th><th rowspan="2">16-shot 79.4</th></tr><tr><th>Table Schema</th><th>Table(Columns)</th><th></th></tr></thead><tbody><tr><td></td><td>Columns=[]</td><td></td><td>69.1</td><td>70.7</td><td>74.4</td><td>77.8</td><td>79.5</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>71.2</td><td>734</td><td>75.4</td><td>78.4</td><td>80.0</td></tr><tr><td>CreateTable</td><td></td><td>71.7</td><td>73.1</td><td>75.8</td><td>78.0</td><td>79.5</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.8</td><td>72.8</td><td>76.6</td><td>79.1</td><td>81.6</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>72.1</td><td>73.3</td><td>76.4</td><td>78.9</td><td>81.3</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>73.6</td><td>73.8</td><td>76.8</td><td>79.8</td><td>814</td></tr></tbody></table>\n\nTable 5: Single-domain results of ChatGPT using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined.'), Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574404+07:00'}, page_content='List of Titles: How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings, The Ohio State University, Abstract, 1 Introduction, 2 In-context Learning for Text-to-SQL, 3 Prompt Construction, 3.1 Database Prompt, InsertRow (Chen et al., 2023), 3.2 Demonstration Prompt, 4 Experiments, 5 Results, 5.1 Zero-shot Text-to-SQL, 5.2 Single-domain Text-to-SQL, Q3: How does Codex perform compared to, 5.3 Cross-domain Text-to-SQL, 5.3.1 Impact of Demonstration Prompt, 5.3.2 Impact of Database Prompt, 6 Related Work, 7 Conclusions, Limitation, Ethics Statement, References, A Appendix, A.1 Prompt Examples, Zero-shot normalized, of the, t2.id, t3, select, Mexico City 250 2, DP/GT, April 19, all, A.2 Tests of Significance, A.3 Detailed Single-domain Results, A.4 TImpact of Demonstration Prompt for ChatGPT-16K\n\n2023\n\n\nHow to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings\n\nShuaichen Chang and Eric Fosler-Lussier\n\n\nThe Ohio State University\n\n{chang.1692, fosler-lussier.1} @osu.edu\n\n\nAbstract\n\nLarge language models (LLMs) with in-context learning have demonstrated remarkable ca- pability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and interme- diate reasoning steps to enhance the perfor- mance of LLMs. However, those works of- ten employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contri- butions. Furthermore, selecting an effective prompt construction has emerged as a persis- tent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across var- ious settings and provide insights into prompt constructions for future text-to-SQL studies. ! Database CREATE TABLE Highschooler ( ID int primary key, name text, grade int )5 /% 3 example rows: SELECT * FROM Highschooler LIMIT 3; name grade Jordan 9 Gabriel 9 1381 Tiffany 9 x/ Task Instruction -- Using valid SQLite, answer the following questions for the tables provided above. Demonstration Question: What is Kyle\'s id? SELECT ID FROM Highschooler WHERE name = " Kyle”; Test Question Question: How many high schoolers are there? SELECT\n\n\n1 Introduction\n\nText-to-SQL models enable users to query databases using natural language questions (NLQs) without having to develop the underlying SQL query. Over the past few decades, neural models with supervised learning have achieved impressive performance on the text-to-SQL task, which are usually trained on a large training set and then eval- uated on test examples (Wang et al., 2019; Yu et al., 2021; Rubin and Berant, 2021; Scholak et al., 2021; Gan et al., 2021; Li et al., 2023a). Figure 1: An example of prompt text for 1-shot single- domain text-to-SQL using a snippet of the database Network_1 with a question from the Spider dataset (Yu etal., 2018). in-context learning allows LLMs to convert a test NLQ into a SQL query using a prompt text. This prompt text includes essential components such as the test database and question. These are accompanied by zero or a few demonstrations: NLQ-SQL pairs corresponding to either the test database (single-domain) or different databases (cross-domain). Figure 1 provides an example of a prompt text for a one-shot single-domain task. Recently, large language models (LLMs) have demonstrated strong capabilities for in-context learning on many language understanding and gen- eration tasks (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022), including on the text-to-SQL task (Rajkumar et al., 2022; Chang et al., 2023; Liu et al., 2023). Instead of train- ing a text-to-SQL model on a large training set,\n"The code for the paper is available at https://github. com/shuaichenchang/prompt-text-to-sql. Previous research has augmented the text-to- SQL capability of LLMs with demonstration- retrieval strategies (Poesia et al., 2022; Shi et al., 2022), intermediate reasoning steps (Cheng et al., 2022; Chen et al., 2023; Pourreza and Rafiei, 2023), and self-debugging ability (Chen et al., 2023; Pour- reza and Rafiei, 2023). However, those studies of- ten employ different prompt strategies that include various key components of text-to-SQL: database\nschema and content, and demonstration examples. The difference in prompt constructions makes it difficult to directly compare two studies on their main contribution, and the outcomes of different studies may change based on future revelations in prompt engineering. In this paper, we evaluate various strategies for prompt construction in three commonly employed text-to-SQL settings: zero-shot, single-domain, and cross-domain. We assess LLMs on text-to- SQL, considering various database prompt con- structions in all three settings. Additionally, in the cross-domain scenario, we investigate the strategy for constructing demonstrations. Through our eval- uation, we aim to gain insights into the effective- ness of these prompt construction strategies. Our findings can be summarized as follows:\n« Table relationship and table content play a crucial role in effectively prompting LLMs. However, it is essential to carefully consider their repre- sentation in the prompt, as LLMs are sensitive to the specific presentation in the zero-shot and cross-domain settings. In-domain demonstration examples can mitigate LLMs’ sensitivity to different representations of database knowledge but they cannot replace table content knowledge. The length of the prompt has a significant impact on the LLMs’ performance in the cross-domain setting. We discovered a preferred prompt length that leads to improved performance. 2 In-context Learning for Text-to-SQL\n\nIn the text-to-SQL task, a database and a natural language question (NLQ) are provided as input for generating an output SQL query. Traditional supervised learning approaches train models on specific text-to-SQL datasets. However, in-context learning allows pretrained large language models (LLMs) to perform text-to-SQL by providing either zero or a few training examples (NLQ-SQL pairs) as demonstrations. This section introduces three widely used settings for in-context learning in text- to-SQL. Prompt examples in these settings can be found in Appendix A.1. Zero-shot Text-to-SQL This setting evaluates the text-to-SQL capability of pretrained LLMs to directly infer the NLQ-SQL relationship from a table without any demonstration examples. The input includes a task instruction and a test question\nwith its corresponding database. Zero-shot text- to-SQL is used to directly assess the text-to-SQL capability of LLMs (Rajkumar et al., 2022; Chang et al., 2023; Liu et al., 2023). Single-domain Few-shot Text-to-SQL This set- ting is designed for applications or domains where it is easy to construct examples, such as booking flights (Price, 1990; Dahl et al., 1994) and querying geographic information (Zelle and Mooney, 1996). It tests the ability of LLMs to adapt with a few in-domain demonstration examples, which are col- lected from the same database as the test question. The goal is to evaluate how well the LLMs can per- form text-to-SQL with minimal in-domain training data (Rajkumar et al., 2022). Cross-domain Few-shot Text-to-SQL This set- ting evaluates the generalization capability of mod- els to new domains by learning from out-of-domain demonstrations. In this scenario, the demonstra- tion NLQ-SQL pairs correspond to one or multiple demonstration databases that are different from the test database. Cross-domain few-shot text-to-SQL assesses how well LLMs can apply their learned knowledge from demonstrations to new databases (Poesia et al., 2022; Chen et al., 2023). 3 Prompt Construction\n\nA text-to-SQL prompt typically comprises four components: a task instruction, a test database, a test NLQ, and optional demonstrations, as il- lustrated in Figure 1. While the task instruction and test NLQ are easily presented in natural lan- guage, there are various strategies for representing the databases and incorporating demonstrations. In this section, we explore different prompt construc- tions for databases and demonstrations. 3.1 Database Prompt\n\nA relational database consists of the database schema and database content. The database schema encompasses the schemas (headers) of tables and the relationship among tables, and database content refers to the data stored in the tables. Database Schema Figure 2 illustrates various prompt constructions for the database schema that have been utilized in previous studies: (1) Table(Columns) (Liu et al., 2023) lists each table along with its columns inside parentheses to repre- sent the table schemas; (2) Columns=[] (Pourreza and Rafiei, 2023) represents each table along with\n<table><tbody><tr><td>Highschooler (ID, name, grade); Friend(studen d, frien</td></tr><tr><td>Columns=[] (Pourreza and Rafiei, 2023)</td></tr><tr><td>Table Highschooler, Columns gradel; Table Friend, Columns = [student_id, friend_id];</td></tr><tr><td>+FK (Pourreza and Rafiei, 2023)</td></tr><tr><td>Foreign_keys = [Friend.student_id Highschooler.ID, Friend.friend_id Highschooler.ID];</td></tr><tr><td>CreateTable (Rajkumar et al., 2022)</td></tr><tr><td>CREATE TABLE Highschooler ( ID int primary key, name text, grade int</td></tr><tr><td>)5 CREATE TABLE Friend (</td></tr><tr><td>student_id int, friend_id int,</td></tr><tr><td>primary key (student_id,friend_id), foreign key(student_id) references Highschooler (ID),</td></tr><tr><td>foreign key (friend_id) references Highschooler (ID)</td></tr></tbody></table>\n\nFigure 2: Examples of the different database schema constructions for a snippet of database Network_1 in Spider. alist of its columns using an equation-like notation; (3) +ForeignKey (Pourreza and Rafiei, 2023) fur- ther adds foreign keys to indicate the relationships between tables; (4) CreateTable (Rajkumar et al., 2022) employed the “Create Table” statement to display the table schemas and relationships. To ensure consistency in the prompt text and accommodate the case-insensitivity of SQL key- words and the database schema, we unify the space and line break in the prompt text and convert all words to lowercase, except for the database content. This normalization process helps to standardize the prompt text. An example is shown in Figure 4. Database content Previous research shows that being aware of database content can improve model performance by exposing models to the specific format of values in each column (Wang et al., 2019; Lin et al., 2020; Scholak et al., 2021; Rajkumar et al., 2022). For instance, the phrase “American student” could be converted to “WHERE country = ‘USA’” or “WHERE country = ‘The United States of America’” depending on the contents of the country column. Figure 3 summarizes different approaches used to construct prompts for showcasing the content of a database. (1) InsertRow (Chen et al., 2023):\n\n\nInsertRow (Chen et al., 2023)\n\n<table><tbody><tr><td>INSERT INTO</td><td>Highschooler</td><td>(ID,</td><td>name,</td><td>grade)</td></tr><tr><td colspan="5">VALUES (1510, "Jordan”, 9);</td></tr><tr><td colspan="5">INSERT INTO Highschooler (ID, name, grade)</td></tr><tr><td colspan="5">VALUES (1689, "Gabriel”, 9);</td></tr><tr><td>INSERT INTO VALUES (1381,</td><td>Highschooler "Tiffany”,</td><td>(ID, 9);</td><td>name,</td><td>grade)</td></tr><tr><td colspan="5">SelectRow (Rajkumar et al., 2022)</td></tr><tr><td colspan="5">/%</td></tr><tr><td colspan="5">3 example rows:</td></tr><tr><td>SELECT * FROM</td><td>Highschooler</td><td></td><td>LIMIT 3;</td><td></td></tr><tr><td colspan="5">ID name grade</td></tr><tr><td>1510 Jordan</td><td>9</td><td></td><td></td><td></td></tr><tr><td>1689 Gabriel</td><td>9</td><td></td><td></td><td></td></tr><tr><td colspan="5">1381 Tiffany 9 */</td></tr><tr><td colspan="5">/% Columns in Highschooler and 3 distinct examples in each column:</td></tr><tr><td>ID: 1025, 1101, name: "Jordan”, grade: 9, 10, x/</td><td>1247 "Gabriel”, 11</td><td></td><td>"Tiffany"”</td><td></td></tr></tbody></table>\n\nFigure 3: Examples of the different database content constructions for showing 3 cell values in each column for the Highschool table in Figure 2. This method displays R rows of each table by utiliz- ing R “INSERT INTO” statements. (2) SelectRow (Rajkumar et al., 2022): This approach employs the “SELECT * FROM Table LIMIT R” query to display the first R rows of each table. (3) SelectCol: In- stead of presenting table content in a row-wise man- ner, an alternative method is to use a column-wise format. As there may be duplicated content across different rows, presenting the content column-wise ensures the provision of distinct values within each column to expose LLMs to a broader range of content. We propose using the query “SELECT DISTINCT [Column] FROM [Table] LIMIT R” to list R distinct cell values in each column. 3.2 Demonstration Prompt\n\nIn few-shot settings, LLMs are provided with demonstrations within the prompt text. In the single-domain few-shot setting, we incorporate a few pairs of NLQs and SQLs as demonstrations inserted between the test database and question, following previous work (Rajkumar et al., 2022). In the cross-domain few-shot setting, we use both out-of-domain NLQ-SQL pairs (demonstration ex- amples) and corresponding databases (demonstra- tion databases) placed before the test database and question. Prior research in the /N-shot setting either uses one demonstration database with /N examples (Pourreza and Rafiei, 2023) or employs N demon- stration databases, each with a single NLQ-SQL\n<table><thead><tr><th>Unnormalized database</th><th>and SQL</th><th></th><th></th></tr></thead><tbody><tr><td>-- Database Schema CREATE TABLE Highschooler ID int primary name text, grade int);</td><td>( key,</td><td></td><td></td></tr><tr><td>-- SQL Query SELECT count( % ) FROM Name = "Kyle";</td><td>Highschooler</td><td>WHERE</td><td></td></tr><tr><td colspan="4" rowspan="2">Normalized database and SQL -- Database Schema</td></tr><tr><td colspan="4">create table highschooler ( id int primary key, name text, grade int ) -- SQL Query</td></tr><tr><td>select count(*) from = \'Kyle\';</td><td>highschooler</td><td>where</td><td>name</td></tr></tbody></table>\n\nFigure 4: An example of the normalization for database and SQL prompts. pair (Poesia et al., 2022; Chen et al., 2023). In con- trast, we consider a more general scenario where the demonstrations comprise M databases, each with K’ NLQ-SQL pairs, with M x K = N. We list the examples of 4-shot single-domain and cross- domain demonstrations in Appendix A.1. Additionally, we normalize demonstration SQL queries by first parsing the SQL queries and unify- ing their format, such as using lowercase for SQL keywords and database schema and unifying the space around punctuation. Figure 4 provides an example of SQL normalization. 4 Experiments\n\nData & Evaluation For our experiments, we uti- lize the Spider dataset (Yu et al., 2018), a cross- domain benchmark for the text-to-SQL task. We conduct our experiments on the development set of Spider (Spider-dev) as the test set is not publicly available. Spider-dev consists of 20 databases with 1034 pairs of NLQ and SQL in total. We evaluate models with execution accuracy (EX) which com- pares the execution results of a predicted SQL and a gold SQL. In the cross-domain setting, we use the train- ing set of Spider to select demonstration exam- ples. As a few databases contain long schema that may cause the prompt to exceed the token limits of LLMs, we only use the databases with fewer than 1000 tokens when constructing the CreateTable prompt. This results in a total of 130 databases being used as demonstration databases in the cross- domain setting. Models We used GPT-3 Codex (Chen et al., 2021a) and ChatGPT due to their demonstrated performance and prevalence in the field.>\nExperiment Setup For the zero-shot setting, we construct each prompt text with a task instruction, a test database, and a test question. We include R = 3 table rows in the database prompt, which has been discovered as the optimal number in previ- ous work (Rajkumar et al., 2022). For the few-shot settings, we incorporate /N demonstration exam- ples in addition to the zero-shot prompt text. In the single-domain text-to-SQL scenario, we use a leave-one-out split, as some databases in Spider-dev contain a small number of examples. ‘When evaluating one example, we regard all other examples from the same database as the training set and randomly retrieve N examples from them. Since Spider contains multiple NLQs correspond- ing to the same SQL query, we require that the train- ing set does not contain examples with the same SQL template as the test example, again following previous work (Finegan-Dollak et al., 2018). In the cross-domain scenario, we randomly se- lect M demonstration databases, each with K NLQ-SQL pairs (M x K = N) from the Spider training set. Incorporating multiple demonstration databases in a prompt text significantly increases its length. Hence, we only use Codex for the cross- domain experiments, due to its higher token limit of 8K, surpassing the 4K limit of ChatGPT. In both single-domain and cross-domain settings, we com- pare different prompt construction methods using the same few-shot examples to make a fair compar- ison. We repeat our experiments three times and present the average results. 5 Results\n\nIn this section, we present our empirical findings in the areas of zero-shot, single-domain, and cross- domain text-to-SQL. Through our experiments, we aim to answer a few crucial research questions in each setting and provide insightful strategies for future studies on effective prompting. 5.1 Zero-shot Text-to-SQL\n\nIn the zero-shot setting, we focus on comparing different prompt constructions for databases. Table\n2We employ the Code-davinci-002 version of Codex across all settings. In zero-shot and single-domain setups, we uti- lize the gpt-3.5-turbo-0301 version of ChatGPT. For cross- domain experiments involving ChatGPT-16K, we turned to gpt-3.5-turbo-16k-0613 due to its extended context length. <table><thead><tr><th></th><th></th><th></th><th colspan="2">Codex</th><th colspan="2">ChatGPT</th></tr><tr><th colspan="2">Database Prompt Construction</th><th></th><th># Tokens (UIN)</th><th>EX (UIN)</th><th># Tokens (UIN)</th><th>EX (UIN)</th></tr></thead><tbody><tr><td rowspan="2">Table Schema</td><td>Table(Columns)</td><td></td><td>1481147</td><td>69.0171.9</td><td>1181115</td><td>68.8170.5</td></tr><tr><td>Columns=[]</td><td></td><td>1691167</td><td>70.2171.8</td><td>1371135</td><td>68.3169.1</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>2261223</td><td>723173.1</td><td>1781174</td><td>729171.2</td></tr><tr><td>CreateTable</td><td></td><td>4741356</td><td>71.8173.1</td><td>3391254</td><td>70.7171.7</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>108911013</td><td>709171.9</td><td>964 | 872</td><td>7181718</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>8201770</td><td>733174.1</td><td>7611674</td><td>71.8172.1</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>9581831</td><td>75.0175.7</td><td>7991712</td><td>733173.6</td></tr></tbody></table>\n\nTable 1: Zero-shot results of Codex and ChatGPT using different database prompt constructions. Table Schema (upper part) contains prompts that solely include the schema of tables, while +Relationship (middle part) incor- porates foreign keys as the table relationships and +Relationship+Content (lower part) adds table content as well. # Tokens is the average token counts in the prompts and EX represents the execution accuracy of SQLs. UIN represents the results of unnormalized prompts and normalized prompts, respectively. The underlines highlight the lower number of tokens and higher accuracies when comparing unnormalized and normalized prompts and the highest accuracy achieved among all prompts is highlighted in bold. 1 shows the average prompt length and execution accuracy of Codex and ChatGPT using various database prompt constructions. Q1: How does normalized database prompt perform compared to unnormalized ones? Nor- malized schemas are found to have a reduced to- ken count in comparison to unnormalized schemas across all database constructions. The normaliza- tion also tends to yield slightly better performance. As for Codex, normalized schemas show improve- ment in all prompts. For ChatGPT, normalized schemas either improve accuracy or achieve the same accuracy or achieve the same level of accu- racy as unnormalized schemas in 6 out of 7 schema constructions. The tests of statistical significance are presented in Appendix A.2. Q2: What database knowledge is crucial for effectively prompting LLMs? Our experiments indicate that table relationships and content are im- portant. The Columns=[] prompt includes only the table schema, while the Columns=[]J+ForeignKey prompt contains the additional relationship among tables shown as foreign keys. Including such information improves the performance of both Codex (71.8 -> 73.1) and ChatGPT (69.1 -> 71.2). Moveover, exposing LLMs to database content with the SelectRow and SelectCol prompts fur- ther enhances the performance of both Codex and ChatGPT, while the InsertRow prompt does not seem to be beneficial. We believe that database content is valuable, but its representation needs to be carefully chosen. ChatGPT? While we do not focus on comparing different LLMs on the text-to-SQL tasks in this paper, it is worth noting that Codex consistently outperforms ChatGPT on zero-shot text-to-SQL using various prompt constructions. Based on all the findings above, we would recom- mend using Codex in conjunction with normalized CreateTableSelectCol prompt construction for zero-shot text-to-SQL.3\n\n\n5.2 Single-domain Text-to-SQL\n\nIn the zero-shot text-to-SQL setting, we discovered that the prompt constructions of databases impact the performance of LLMs. This discovery naturally raises the question of whether the introduction of in-domain demonstrations affects the performance of LLMs to different database prompts. Q1: Does the use of in-domain demonstrations enhance LLM’s performance? Figure 5 depicts the performance of Codex and ChatGPT using dif- ferent database prompt constructions with respect to different numbers of in-domain demonstration examples. For all database prompts, the perfor- mance of LLMs experiences a notable improve- ment when in-domain examples are presented. Fur- thermore, the performance continues to enhance as the number of in-domain examples increases. Q2: What database knowledge is important when presenting in-domain demonstrations? While we have observed that the presence of table\n\n\nQ3: How does Codex perform compared to\n\n5 R . . “To simplify our experiments and ensure consistent prompts, we adopt normalization for single-domain and cross- domain experiments. ###IMAGE###Figure-5###IMAGE###\nFigure 5: Execution accuracy of Codex and ChatGPT for single-domain text-to-SQL with 1, 4, 8, and 16 in- domain examples. RS and Cont correspond to table relationship and table content, respectively.'), Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574436+07:00'}, page_content='Detailed results can be found in Table 4 and 5. relationships and table content enhanced LLMs’ performance in the zero-shot scenario, it is not clear whether they are still important in the single- domain setting. A hypothesis is that table relation- ship and table content knowledge can be acquired from in-domain examples as they may appear in SQL clauses JOIN and WHERE. For table relationships, we compare two database prompt constructions Columns=[] and Columns=[]+ForeignKey. Both construct the ta- ble schema in the same way while the latter in- cludes foreign keys as table relationships. In the zero-shot scenario, Columns=[]+ForeignKey out- performs Columns=[] by 1.3 and 2.1 for Codex and ChatGPT, respectively. However, as increasing the number of in-domain examples, we notice a gradual reduction in the performance gap between these two prompts. With the utilization of 16 in- domain examples, the gap completely disappears for Codex, while ChatGPT exhibits a marginal dif- ference of only 0.5%. For table content, we compare CreateTable with CreateTable+SelectCol. Both contain the same prompts for presenting the table schema and relationship, while the latter addi- tionally includes table content. In the zero-shot\nscenario, CreateTable+SelectCol outperforms CreateTable by 2.0% for Codex and 1.7% for ChatGPT. As we proceed to increase the number of in-domain examples, we observe that the per- formance gap between these two prompts does not exhibit a significant reduction. Even with 16 in- domain examples, the gap still persists at 1.3 for Codex and 1.9 for ChatGPT. These results indicate LLMs are able to quickly learn table relationships from a small number of in- domain demonstrations, however, it is more chal- lenging to obtain table content knowledge from demonstration examples. Consequently, the inclu- sion of table content remains crucial for achieving satisfactory performance in the single-domain text- to-SQL scenario. Q3: Can in-domain demonstrations alleviate the sensitivity of LLMs to the representation of ta- ble content? In the zero-shot setting, we observe that LLMs are sensitive to how the table content is presented. Specifically, SelectCol 3 outperforms InsertRow 3 by a substantial margin of 3.8 for Codex and 1.8 for ChatGPT. However, as we ex- pose LLMs to in-domain demonstrations, LLMs be- come less sensitive to the specific representation of table content. The performance disparities among the three table content prompts become marginal. Notably, with only 4 examples, the performance difference between SelectCol 3 and InsertRow 3 diminishes to 0.3 for Codex and 0.2 for ChatGPT. To summarize, in single-domain text-to-SQL, we recommend incorporating a greater number of in-domain examples whenever feasible. It is also essential to ensure the presence of table content in conjunction with the table schema while the specific choice of table content construction is less crucial compared to the zero-shot scenario. 5.3 Cross-domain Text-to-SQL\n\nIn this section, we present the results to answer a series of questions regarding the demonstration and database prompt construction. 5.3.1 Impact of Demonstration Prompt\n\nTo investigate the impact of the number of databases and examples per database in demon- strations, we conduct experiments encompassing various combinations. Specifically, our demonstra- tions are composed of M demonstration databases, each containing K NLQ-SQL pairs. We consider scenarios with up to 8 databases and 16 examples per database as long as the combination does not\n###IMAGE###Figure-6###IMAGE###\nFigure 6: A heat map of Codex’s execution accuracy us- ing CreateTable+SelectRow 3 for different numbers of databases and examples per database in the demon- stration. Darker color indicates higher accuracy. ###IMAGE###Figure-6###IMAGE###\n2000 3000 4000 5000 6000 7000 # Prompt Tokens\nFigure 7: Execution accuracy of Codex in relation to the length of prompts. Each dot on the graph represents a specific demonstration prompt construction, with the m, k denoting the number of databases and examples per database used in the prompt. The lines represent second-degree polynomial trendlines fitted to the results. exceed the prompt length limit. We opt to use the database prompt CreateTable+SelectRow 3 as it contains fewer tokens compared to InsertRow and SelectCol while encompassing all valuable database knowledge. We present the experiments with Codex in this section. Experiments involv- ing ChatGPT-16K can be found in Appendix A.4 which show similar results as Codex. Q1: Does increasing demonstration examples enhance LLMs’ performance? Figure 2 presents the accuracy of Codex corresponding to different combinations of the number of databases and the number of examples per database used as demon- strations. We analyze the results from two perspec- tives. Firstly, for a fixed number of databases, we observe an initial improvement in Codex’s perfor- mance as the number of examples per database increases. However, this improvement plateaus or declines once 4 examples per database are provided. Surprisingly, when using 4 databases, employing 8\nor 16 examples per database leads to a significant decrease in the Codex’s performance compared to using 2 or 4 examples per database. Secondly, for a fixed number of examples per database, we ob- serve an initial increase in Codex’s performance as the number of databases increases, however, this improvement is followed by a significant decrease once the number of databases reaches a certain threshold (either 4 or 6). Q2: Why does increasing the number of databases decrease LLMs’ performance? As depicted in Figure 2, presenting more databases does not always lead to improved performance. In fact, there is a significant decline in performance, once it surpasses a threshold. We hypothesize that this phenomenon is attributed to the length of the prompt text. To test this hypothesis, we analyze the results in relation to the prompt length. Figure 7 shows the relationship between the accuracy of different demonstration prompts and their prompt lengths. Notably, the performance of Codex exhibits an inverted-U shape as the prompt length increases for each number of examples per database. Additionally, we observe a substantial drop in performance once the prompt text length exceeds approximately 5500 tokens. Similarly, Fig- ure 9 shows that the performance of ChatGPT-16K starts to decrease when prompt text length exceeds 11K tokens. Based on these observations, we con- jecture that LLMs may have a sweet spot in terms of prompt length, potentially influenced by factors such as their model architecture or training data. This indicates that even though LLM:s are capable of handling long contexts, they may not necessarily perform better with excessively long prompts. 5.3.2 Impact of Database Prompt\n\nSince incorporating demonstration databases may cause a decrease in Codex’s performance, we fo- cus our database prompt experiments on using one demonstration database in combination with vary- ing quantities of demonstration examples. Table 2 presents the execution accuracy of Codex using different database prompts. Q3: Do different database prompts show similar trends with the number of demonstration exam- ples? We observe an initial performance increase for all database prompts. However, once more than 4 examples are provided, the improvement starts to level off, indicating that the different database prompts exhibit similar trends in relation to the number of demonstration examples. <table><thead><tr><th rowspan="2">Database Prompt Table Schema</th><th>Construction</th><th></th><th>0-shot</th><th>1-shot</th><th>2-shot</th><th>4-shot</th><th>8-shot</th><th>16-shot</th></tr><tr><th>Table(Columns)</th><th></th><th>71.9</th><th>72.0</th><th>73.0</th><th>732</th><th>72.8</th><th>73.9</th></tr></thead><tbody><tr><td></td><td>Columns=[]</td><td></td><td>71.8</td><td>71.9</td><td>73.6</td><td>742</td><td>73.7</td><td>744</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>73.1</td><td>733</td><td>74.5</td><td>74.9</td><td>74.9</td><td>75.2</td></tr><tr><td>CreateTable</td><td></td><td>73.1</td><td>72.1</td><td>734</td><td>73.7</td><td>74.1</td><td>75.1</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.9</td><td>722</td><td>74.1</td><td>74.9</td><td>74.9</td><td>74.8</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>74.1</td><td>73.0</td><td>75.0</td><td>76.2</td><td>75.7</td><td>76.0</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>75.7</td><td>74.4</td><td>75.5</td><td>76.5</td><td>76.8</td><td>76.5</td></tr></tbody></table>\n\nTable 2: Cross-domain results of Codex using different database prompt constructions. Only one demonstration database is included in a prompt, N-shot represents N examples corresponding to the demonstration database. The best and second-best results for each shot are highlighted in bold and underlined. Q4: Can out-of-domain demonstrations allevi- ate the sensitivity of LLMs to database prompts? First, we observe that incorporating table relation- ships and content in the prompts remains crucial for effectively prompting Codex in the cross-domain setting. This is not surprising, as Codex cannot di- rectly learn knowledge specific to the test database from the out-of-domain demonstrations. Further- more, we find that Codex continues to exhibit sensi- tivity to the representation of table content. Despite having demonstration databases that mirror the con- struction of the test database, Codex still displays a preference forSelectRow and SelectCol when presenting table content, compared to InsertCol. In conclusion, while out-of-domain demonstra- tions enhance LLMS’ capabilities in text-to-SQL, they do not provide database-specific knowledge. Consequently, careful construction of database prompts remains crucial, aligning with the observa- tions made in the zero-shot setting. 6 Related Work\n\nLLMs for Text-to-SQL In recent years, there has been significant progress in leveraging LLMs for the text-to-SQL task. Various methods have been proposed to enhance the capabilities of LLMs. For example, Rubin et al.')]
(min_rag) minkhant@MinKhant:~/Documents/MyFolder/freelance/RAG_APP/src$ python3 test.py 
{'userId': 'user123', 'pdf': [{'pdf_filename': 'pdf1', 'pdf_title': None, 'image_path': []}], 'chat_history': [{'query': 'What is the capital of India?', 'response': None, 'timestamp': {'query': '2024-11-17 15:22:12', 'response': None}, 'input_tokens': None, 'output_tokens': None}]}
{'userId': 'user123', 'pdf': [{'pdf_filename': 'pdf1', 'pdf_title': None, 'image_path': []}], 'chat_history': [{'query': 'What is the capital of India?', 'response': 'New response', 'timestamp': {'query': '2024-11-17 15:22:12', 'response': '2024-11-17 15:22:12'}, 'input_tokens': None, 'output_tokens': None}]}
[Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574563+07:00'}, page_content='1996. Learn- ing to parse database queries using inductive logic programming. In Proceedings of the Thirteenth Na- tional Conference on Artificial Intelligence - Volume 2, pages 1050-1055. A Appendix\n\n\n\nA.1 Prompt Examples\n\nBelow contains an example of a zero-shot nor- malized prompt, which contains the database Network_1 from Spider (Yu et al., 2018), a task instruction “Using valid SQLite, answer the fol- lowing questions for the tables provided above.”, and a test question “How many high schoolers are there?”. Zero-shot normalized\n\nprompt create table highschooler ( id int primary key, name text, grade int 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /* 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) 3 example rows: select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite, answer the questions for the tables provided Question: How many high schoolers select\nfollowing above. are there? Below contains an example of a 4-shot single- domain normalized prompt, which contains a database prompt and 4 demonstration examples ahead of the test question. 4-shot single-domain normalized prompt create table highschooler ( id int primary key, name text, grade int )5 /% 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 x/ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /% 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 x/ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) )5 /% 3 example rows: select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 x/ -- Using valid SQLite, answer the questions for the tables provided above. Question: What is Kyle\'s id? select id from highschooler where name Kyle\'; Question: Return the names of friends high school student Kyle. select t3.name from friend as t1 join highschooler as t2 on t1.student_id = join highschooler as t3 on t1.friend_id .id where t2.name = \'Kyle\'; Question: Show names of all high school students who do not have any friends. select name from highschooler except t2.name from friend as t1 join highschooler as t2 on tl.student_id = t2.id;\nfollowing\n=\n\n\nof the\n\n\n\nt2.id\n\n=\n\n\nt3\n\n\n\nselect\n\nQuestion: What are the names and grades for each high schooler? select name, grade from highschooler; Question: How many high schoolers are there? select\nBelow contains an example of a 4-shot cross- domain prompt, which contains 2 demonstration databases, each with 2 demonstration examples ahead of the test database and question. cross-domain prompt create table publication ( publication_id int, book_id int, publisher text, publication_date text, price real, primary key (publication_id), foreign key (book_id) references book( book_id) )5 /* 3 example rows: select * from publication limit 3; publication_id book_id publisher publication_date price 1 1 Pearson August 2008 15000000.0 2 3 Thomson Reuters March 2008 6000000.0 3 4 Wiley June 2006 4100000.0 */ create table book ( book_id int, title text, issues real, writer text, primary key (book_id) )5 /* 3 example rows: select x from book limit 3; book_id title issues writer 1 The Black Lamb 6.0 Timothy Truman 2 Bloody Mary 4.0 Garth Ennis 3 Bloody Mary : Lady Liberty 4.0 Garth Ennis */ -- Using valid SQLite, answer the following questions for the tables provided above. Question: List the writers of the books in ascending alphabetical order. select writer from book order by writer asc; Question: How many books are there? select count(*) from book; create table race ( race_id int, name text, class text, date text, track_id text, primary key (race_id), foreign key (track_id) references track( track_id) )5 /* 3 example rows: select x from race limit 3; race_id name class date track_id 1 Rolex 24 At Daytona DP/GT January 26 January 27 1 2 Gainsco Grand Prix of Miami DP/GT March 29 2\n3\n\n\nMexico City 250 2\n\n\n\nDP/GT\n\n\n\nApril 19\n\n/\ncreate table track ( track_id int, name text, location text,\nseating real, year_opened real, primary key (track_id) )5 /% 3 example rows: select * from track limit 3; track_id name location seating year_opened 1 Auto Club Speedway Fontana, CA 92000.0 1997.0 2 Chicagoland Speedway Joliet, IL 75000.0 2001.0 3 Darlington Raceway Darlington, SC 63000.0 1950.0 x/ -- Using valid SQLite, answer the following questions for the tables provided above. Question: Show the name and location for tracks . select name, location from the track; Question: Show the name of track and the number of races in each track. select t2.name, count(*) from race as t1 join track as t2 on tl.track_id = t2. track_id group by t1.track_id; create table highschooler ( id int primary key, name text, grade int )5 /% 3 example rows: select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 x/ create table friend ( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references highschooler (id), foreign key (friend_id) references highschooler (id) )5 /% 3 example rows: select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 x/ create table likes ( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references highschooler (id), foreign key (student_id) references highschooler (id) )5 /%\n3 example rows: select * from likes limit 3; student_id liked_id 1689 1709\n1709 1782 x/\n1689 1709\n-- Using valid SQLite, answer the following questions for the tables provided above. Question: How many high schoolers are there? select\n\n\nall\n\n\n\nA.2 Tests of Significance\n\nTable 1 contains the performance of Codex and ChatGPT using different database prompt construc- tions in the zero-shot setting. We observe that the normalization results in slightly improved perfor- mance for all database prompt constructions with Codex and 6 out of 7 database prompt construc- tions with ChatGPT. It is important to note, how- ever, that when comparing normalized and unnor- malized database prompt constructions using the same method, the results did not demonstrate statis- tical significance in McNemar’s test, with p-values greater than 0.05. Nevertheless, the primary advan- tage of normalization lies in its ability to reduce variations among different databases and minimize the overall prompt length. When evaluating various prompt structions, we note the advantages gained from incorporating both table relationships (Columns=[]+ForeignKey vs Columns=[]) and table content (CreateTable+SelectCol 3 vs CreateTable) are mostly statistically significant in McNemar\'’s test, with p-values smaller than 0.05. Table 3 displays the results of the significant tests. The performance of Columns=[]+ForeignKey compared to Columns=[] is statistically sig- nificant in all cases, except for codex with normalized prompts. Likewise, the performance of CreateTable+SelectCol 3 is statistically significant for both Codex and ChatGPT, with both normalized and unnormalized prompts, when com- pared to CreateTable. These significant findings highlight the effectiveness of incorporating table relationships and database content. con-\n\n\nA.3 Detailed Single-domain Results\n\nTables 4 and 5 provide detailed results of Codex and ChatGPT in the single-domain setting, respec- tively. The performance of both models is also illustrated in Figure 5. A.4 TImpact of Demonstration Prompt for ChatGPT-16K\n\nFigure 8 presents the accuracy of ChatGPT-16K corresponding to different combinations of the number of databases and the number of examples per database used as demonstrations. Similar to our findings with Codex, presenting more databases does not always lead to improved performance for ChatGPT-16K. For a fixed number of examples per database, we observe an initial increase in its\n###IMAGE###Figure-8###IMAGE###\nFigure 8: A heat map of ChatGPT-16K’s execution ac- curacy using CreateTable+SelectRow 3 for different numbers of databases and examples per database in the demonstration. Darker color indicates higher accuracy. ###IMAGE###Figure-8###IMAGE###\nFigure 9: Execution accuracy of ChatGPT-16K in re- lation to the length of prompts. Each dot represents a demonstration construction, with the m, k denoting the number of databases and examples per database. The lines represent second-degree polynomial trendlines fit- ted to the results. performance as the number of databases increases, however, this improvement is followed by a de- crease once the number of databases reaches a cer- tain threshold. To understand this phenomenon, we analyze the results in relation to the prompt length. Figure 9 shows the relationship between the ac- curacy of different demonstration prompts and their prompt lengths. Similar to Codex, the performance of ChatGPT-16K also exhibits an inverted-U shape as the prompt length increases for each number of examples per database. Additionally, we observe the performance starts to decrease once the prompt text length exceeds approximately 11K tokens. While Codex supports 8K tokens and ChatGPT- 16K supports 16K tokens, we notice that their performance tends to decline when dealing with demonstrations that exceed approximately 70% of the maximum prompt length. <table><thead><tr><th>Prompt 1</th><th>Prompt 2</th><th>LLM</th><th>Normalization</th><th>Significant Test</th></tr></thead><tbody><tr><td rowspan="4">Columns=[]</td><td rowspan="4">Columns=[J+ForeignKey</td><td>Codex</td><td>U</td><td>v</td></tr><tr><td>Codex</td><td></td><td>%</td></tr><tr><td>ChatGPT</td><td></td><td>N</td></tr><tr><td>ChatGPT</td><td>Zlc|Z|c|z|c|z</td><td>SIS N</td></tr><tr><td rowspan="4">CreateTable</td><td rowspan="4">CreateTable+SelectCol 3</td><td>Codex</td><td></td><td></td></tr><tr><td>Codex</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td></td></tr></tbody></table>\n\nTable 3: Tests of Statistical Significance for comparing different prompt constructions. Prompt 1 and Prompt 2 were used to represent two distinct methods of constructing prompts in McNemar’s test. The prompts were categorized as U and N, representing unnormalized and normalized database prompts, respectively. The v symbol indicates that the p-value is smaller than 0.05, indicating statistical significance, while the X symbol indicates p-values greater than 0.05, indicating a lack of statistical significance. <table><thead><tr><th colspan="2">Database Prompt Construction</th><th></th><th rowspan="2">0-shot 719</th><th rowspan="2">1-shot 70.7</th><th rowspan="2">4-shot 74.9</th><th rowspan="2">8-shot 78.0</th><th rowspan="2">16-shot 81.6</th></tr></thead><tbody><tr><td rowspan="2">Table Schema</td><td>Table(Columns)</td><td></td></tr><tr><td>Columns=[]</td><td></td><td>718</td><td>721</td><td>755</td><td>780</td><td>81.6</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>73.1</td><td>732</td><td>761</td><td>78.5</td><td>81.6</td></tr><tr><td>CreateTable</td><td></td><td>73.1</td><td>724</td><td>760</td><td>787</td><td>81.3</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.9</td><td>72.9</td><td>71.6</td><td>80.5</td><td>82.5</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>74.1</td><td>73.4</td><td>713</td><td>80.5</td><td>82.9</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>75.7</td><td>74.1</td><td>71.9</td><td>80.7</td><td>82.5</td></tr></tbody></table>\n\nTable 4: Single-domain results of Codex using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined. <table><thead><tr><th colspan="2">Database Prompt Construction</th><th></th><th rowspan="2">0-shot 70.5</th><th rowspan="2">1-shot 71.6</th><th rowspan="2">4-shot 74.3</th><th rowspan="2">8-shot 774</th><th rowspan="2">16-shot 79.4</th></tr><tr><th>Table Schema</th><th>Table(Columns)</th><th></th></tr></thead><tbody><tr><td></td><td>Columns=[]</td><td></td><td>69.1</td><td>70.7</td><td>74.4</td><td>77.8</td><td>79.5</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>71.2</td><td>734</td><td>75.4</td><td>78.4</td><td>80.0</td></tr><tr><td>CreateTable</td><td></td><td>71.7</td><td>73.1</td><td>75.8</td><td>78.0</td><td>79.5</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>71.8</td><td>72.8</td><td>76.6</td><td>79.1</td><td>81.6</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>72.1</td><td>73.3</td><td>76.4</td><td>78.9</td><td>81.3</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>73.6</td><td>73.8</td><td>76.8</td><td>79.8</td><td>814</td></tr></tbody></table>\n\nTable 5: Single-domain results of ChatGPT using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined.'), Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574569+07:00'}, page_content=''), Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574404+07:00'}, page_content='List of Titles: How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings, The Ohio State University, Abstract, 1 Introduction, 2 In-context Learning for Text-to-SQL, 3 Prompt Construction, 3.1 Database Prompt, InsertRow (Chen et al., 2023), 3.2 Demonstration Prompt, 4 Experiments, 5 Results, 5.1 Zero-shot Text-to-SQL, 5.2 Single-domain Text-to-SQL, Q3: How does Codex perform compared to, 5.3 Cross-domain Text-to-SQL, 5.3.1 Impact of Demonstration Prompt, 5.3.2 Impact of Database Prompt, 6 Related Work, 7 Conclusions, Limitation, Ethics Statement, References, A Appendix, A.1 Prompt Examples, Zero-shot normalized, of the, t2.id, t3, select, Mexico City 250 2, DP/GT, April 19, all, A.2 Tests of Significance, A.3 Detailed Single-domain Results, A.4 TImpact of Demonstration Prompt for ChatGPT-16K\n\n2023\n\n\nHow to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings\n\nShuaichen Chang and Eric Fosler-Lussier\n\n\nThe Ohio State University\n\n{chang.1692, fosler-lussier.1} @osu.edu\n\n\nAbstract\n\nLarge language models (LLMs) with in-context learning have demonstrated remarkable ca- pability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and interme- diate reasoning steps to enhance the perfor- mance of LLMs. However, those works of- ten employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contri- butions. Furthermore, selecting an effective prompt construction has emerged as a persis- tent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across var- ious settings and provide insights into prompt constructions for future text-to-SQL studies. ! Database CREATE TABLE Highschooler ( ID int primary key, name text, grade int )5 /% 3 example rows: SELECT * FROM Highschooler LIMIT 3; name grade Jordan 9 Gabriel 9 1381 Tiffany 9 x/ Task Instruction -- Using valid SQLite, answer the following questions for the tables provided above. Demonstration Question: What is Kyle\'s id? SELECT ID FROM Highschooler WHERE name = " Kyle”; Test Question Question: How many high schoolers are there? SELECT\n\n\n1 Introduction\n\nText-to-SQL models enable users to query databases using natural language questions (NLQs) without having to develop the underlying SQL query. Over the past few decades, neural models with supervised learning have achieved impressive performance on the text-to-SQL task, which are usually trained on a large training set and then eval- uated on test examples (Wang et al., 2019; Yu et al., 2021; Rubin and Berant, 2021; Scholak et al., 2021; Gan et al., 2021; Li et al., 2023a). Figure 1: An example of prompt text for 1-shot single- domain text-to-SQL using a snippet of the database Network_1 with a question from the Spider dataset (Yu etal., 2018). in-context learning allows LLMs to convert a test NLQ into a SQL query using a prompt text. This prompt text includes essential components such as the test database and question. These are accompanied by zero or a few demonstrations: NLQ-SQL pairs corresponding to either the test database (single-domain) or different databases (cross-domain). Figure 1 provides an example of a prompt text for a one-shot single-domain task. Recently, large language models (LLMs) have demonstrated strong capabilities for in-context learning on many language understanding and gen- eration tasks (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022), including on the text-to-SQL task (Rajkumar et al., 2022; Chang et al., 2023; Liu et al., 2023). Instead of train- ing a text-to-SQL model on a large training set,\n"The code for the paper is available at https://github. com/shuaichenchang/prompt-text-to-sql. Previous research has augmented the text-to- SQL capability of LLMs with demonstration- retrieval strategies (Poesia et al., 2022; Shi et al., 2022), intermediate reasoning steps (Cheng et al., 2022; Chen et al., 2023; Pourreza and Rafiei, 2023), and self-debugging ability (Chen et al., 2023; Pour- reza and Rafiei, 2023). However, those studies of- ten employ different prompt strategies that include various key components of text-to-SQL: database\nschema and content, and demonstration examples. The difference in prompt constructions makes it difficult to directly compare two studies on their main contribution, and the outcomes of different studies may change based on future revelations in prompt engineering. In this paper, we evaluate various strategies for prompt construction in three commonly employed text-to-SQL settings: zero-shot, single-domain, and cross-domain. We assess LLMs on text-to- SQL, considering various database prompt con- structions in all three settings. Additionally, in the cross-domain scenario, we investigate the strategy for constructing demonstrations. Through our eval- uation, we aim to gain insights into the effective- ness of these prompt construction strategies. Our findings can be summarized as follows:\n« Table relationship and table content play a crucial role in effectively prompting LLMs. However, it is essential to carefully consider their repre- sentation in the prompt, as LLMs are sensitive to the specific presentation in the zero-shot and cross-domain settings. In-domain demonstration examples can mitigate LLMs’ sensitivity to different representations of database knowledge but they cannot replace table content knowledge. The length of the prompt has a significant impact on the LLMs’ performance in the cross-domain setting. We discovered a preferred prompt length that leads to improved performance. 2 In-context Learning for Text-to-SQL\n\nIn the text-to-SQL task, a database and a natural language question (NLQ) are provided as input for generating an output SQL query. Traditional supervised learning approaches train models on specific text-to-SQL datasets. However, in-context learning allows pretrained large language models (LLMs) to perform text-to-SQL by providing either zero or a few training examples (NLQ-SQL pairs) as demonstrations. This section introduces three widely used settings for in-context learning in text- to-SQL. Prompt examples in these settings can be found in Appendix A.1. Zero-shot Text-to-SQL This setting evaluates the text-to-SQL capability of pretrained LLMs to directly infer the NLQ-SQL relationship from a table without any demonstration examples. The input includes a task instruction and a test question\nwith its corresponding database. Zero-shot text- to-SQL is used to directly assess the text-to-SQL capability of LLMs (Rajkumar et al., 2022; Chang et al., 2023; Liu et al., 2023). Single-domain Few-shot Text-to-SQL This set- ting is designed for applications or domains where it is easy to construct examples, such as booking flights (Price, 1990; Dahl et al., 1994) and querying geographic information (Zelle and Mooney, 1996). It tests the ability of LLMs to adapt with a few in-domain demonstration examples, which are col- lected from the same database as the test question. The goal is to evaluate how well the LLMs can per- form text-to-SQL with minimal in-domain training data (Rajkumar et al., 2022). Cross-domain Few-shot Text-to-SQL This set- ting evaluates the generalization capability of mod- els to new domains by learning from out-of-domain demonstrations. In this scenario, the demonstra- tion NLQ-SQL pairs correspond to one or multiple demonstration databases that are different from the test database. Cross-domain few-shot text-to-SQL assesses how well LLMs can apply their learned knowledge from demonstrations to new databases (Poesia et al., 2022; Chen et al., 2023). 3 Prompt Construction\n\nA text-to-SQL prompt typically comprises four components: a task instruction, a test database, a test NLQ, and optional demonstrations, as il- lustrated in Figure 1. While the task instruction and test NLQ are easily presented in natural lan- guage, there are various strategies for representing the databases and incorporating demonstrations. In this section, we explore different prompt construc- tions for databases and demonstrations. 3.1 Database Prompt\n\nA relational database consists of the database schema and database content. The database schema encompasses the schemas (headers) of tables and the relationship among tables, and database content refers to the data stored in the tables. Database Schema Figure 2 illustrates various prompt constructions for the database schema that have been utilized in previous studies: (1) Table(Columns) (Liu et al., 2023) lists each table along with its columns inside parentheses to repre- sent the table schemas; (2) Columns=[] (Pourreza and Rafiei, 2023) represents each table along with\n<table><tbody><tr><td>Highschooler (ID, name, grade); Friend(studen d, frien</td></tr><tr><td>Columns=[] (Pourreza and Rafiei, 2023)</td></tr><tr><td>Table Highschooler, Columns gradel; Table Friend, Columns = [student_id, friend_id];</td></tr><tr><td>+FK (Pourreza and Rafiei, 2023)</td></tr><tr><td>Foreign_keys = [Friend.student_id Highschooler.ID, Friend.friend_id Highschooler.ID];</td></tr><tr><td>CreateTable (Rajkumar et al., 2022)</td></tr><tr><td>CREATE TABLE Highschooler ( ID int primary key, name text, grade int</td></tr><tr><td>)5 CREATE TABLE Friend (</td></tr><tr><td>student_id int, friend_id int,</td></tr><tr><td>primary key (student_id,friend_id), foreign key(student_id) references Highschooler (ID),</td></tr><tr><td>foreign key (friend_id) references Highschooler (ID)</td></tr></tbody></table>\n\nFigure 2: Examples of the different database schema constructions for a snippet of database Network_1 in Spider. alist of its columns using an equation-like notation; (3) +ForeignKey (Pourreza and Rafiei, 2023) fur- ther adds foreign keys to indicate the relationships between tables; (4) CreateTable (Rajkumar et al., 2022) employed the “Create Table” statement to display the table schemas and relationships. To ensure consistency in the prompt text and accommodate the case-insensitivity of SQL key- words and the database schema, we unify the space and line break in the prompt text and convert all words to lowercase, except for the database content. This normalization process helps to standardize the prompt text. An example is shown in Figure 4. Database content Previous research shows that being aware of database content can improve model performance by exposing models to the specific format of values in each column (Wang et al., 2019; Lin et al., 2020; Scholak et al., 2021; Rajkumar et al., 2022). For instance, the phrase “American student” could be converted to “WHERE country = ‘USA’” or “WHERE country = ‘The United States of America’” depending on the contents of the country column. Figure 3 summarizes different approaches used to construct prompts for showcasing the content of a database. (1) InsertRow (Chen et al., 2023):\n\n\nInsertRow (Chen et al., 2023)\n\n<table><tbody><tr><td>INSERT INTO</td><td>Highschooler</td><td>(ID,</td><td>name,</td><td>grade)</td></tr><tr><td colspan="5">VALUES (1510, "Jordan”, 9);</td></tr><tr><td colspan="5">INSERT INTO Highschooler (ID, name, grade)</td></tr><tr><td colspan="5">VALUES (1689, "Gabriel”, 9);</td></tr><tr><td>INSERT INTO VALUES (1381,</td><td>Highschooler "Tiffany”,</td><td>(ID, 9);</td><td>name,</td><td>grade)</td></tr><tr><td colspan="5">SelectRow (Rajkumar et al., 2022)</td></tr><tr><td colspan="5">/%</td></tr><tr><td colspan="5">3 example rows:</td></tr><tr><td>SELECT * FROM</td><td>Highschooler</td><td></td><td>LIMIT 3;</td><td></td></tr><tr><td colspan="5">ID name grade</td></tr><tr><td>1510 Jordan</td><td>9</td><td></td><td></td><td></td></tr><tr><td>1689 Gabriel</td><td>9</td><td></td><td></td><td></td></tr><tr><td colspan="5">1381 Tiffany 9 */</td></tr><tr><td colspan="5">/% Columns in Highschooler and 3 distinct examples in each column:</td></tr><tr><td>ID: 1025, 1101, name: "Jordan”, grade: 9, 10, x/</td><td>1247 "Gabriel”, 11</td><td></td><td>"Tiffany"”</td><td></td></tr></tbody></table>\n\nFigure 3: Examples of the different database content constructions for showing 3 cell values in each column for the Highschool table in Figure 2. This method displays R rows of each table by utiliz- ing R “INSERT INTO” statements. (2) SelectRow (Rajkumar et al., 2022): This approach employs the “SELECT * FROM Table LIMIT R” query to display the first R rows of each table. (3) SelectCol: In- stead of presenting table content in a row-wise man- ner, an alternative method is to use a column-wise format. As there may be duplicated content across different rows, presenting the content column-wise ensures the provision of distinct values within each column to expose LLMs to a broader range of content. We propose using the query “SELECT DISTINCT [Column] FROM [Table] LIMIT R” to list R distinct cell values in each column. 3.2 Demonstration Prompt\n\nIn few-shot settings, LLMs are provided with demonstrations within the prompt text. In the single-domain few-shot setting, we incorporate a few pairs of NLQs and SQLs as demonstrations inserted between the test database and question, following previous work (Rajkumar et al., 2022). In the cross-domain few-shot setting, we use both out-of-domain NLQ-SQL pairs (demonstration ex- amples) and corresponding databases (demonstra- tion databases) placed before the test database and question. Prior research in the /N-shot setting either uses one demonstration database with /N examples (Pourreza and Rafiei, 2023) or employs N demon- stration databases, each with a single NLQ-SQL\n<table><thead><tr><th>Unnormalized database</th><th>and SQL</th><th></th><th></th></tr></thead><tbody><tr><td>-- Database Schema CREATE TABLE Highschooler ID int primary name text, grade int);</td><td>( key,</td><td></td><td></td></tr><tr><td>-- SQL Query SELECT count( % ) FROM Name = "Kyle";</td><td>Highschooler</td><td>WHERE</td><td></td></tr><tr><td colspan="4" rowspan="2">Normalized database and SQL -- Database Schema</td></tr><tr><td colspan="4">create table highschooler ( id int primary key, name text, grade int ) -- SQL Query</td></tr><tr><td>select count(*) from = \'Kyle\';</td><td>highschooler</td><td>where</td><td>name</td></tr></tbody></table>\n\nFigure 4: An example of the normalization for database and SQL prompts. pair (Poesia et al., 2022; Chen et al., 2023). In con- trast, we consider a more general scenario where the demonstrations comprise M databases, each with K’ NLQ-SQL pairs, with M x K = N. We list the examples of 4-shot single-domain and cross- domain demonstrations in Appendix A.1. Additionally, we normalize demonstration SQL queries by first parsing the SQL queries and unify- ing their format, such as using lowercase for SQL keywords and database schema and unifying the space around punctuation. Figure 4 provides an example of SQL normalization. 4 Experiments\n\nData & Evaluation For our experiments, we uti- lize the Spider dataset (Yu et al., 2018), a cross- domain benchmark for the text-to-SQL task. We conduct our experiments on the development set of Spider (Spider-dev) as the test set is not publicly available. Spider-dev consists of 20 databases with 1034 pairs of NLQ and SQL in total. We evaluate models with execution accuracy (EX) which com- pares the execution results of a predicted SQL and a gold SQL. In the cross-domain setting, we use the train- ing set of Spider to select demonstration exam- ples. As a few databases contain long schema that may cause the prompt to exceed the token limits of LLMs, we only use the databases with fewer than 1000 tokens when constructing the CreateTable prompt. This results in a total of 130 databases being used as demonstration databases in the cross- domain setting. Models We used GPT-3 Codex (Chen et al., 2021a) and ChatGPT due to their demonstrated performance and prevalence in the field.>\nExperiment Setup For the zero-shot setting, we construct each prompt text with a task instruction, a test database, and a test question. We include R = 3 table rows in the database prompt, which has been discovered as the optimal number in previ- ous work (Rajkumar et al., 2022). For the few-shot settings, we incorporate /N demonstration exam- ples in addition to the zero-shot prompt text. In the single-domain text-to-SQL scenario, we use a leave-one-out split, as some databases in Spider-dev contain a small number of examples. ‘When evaluating one example, we regard all other examples from the same database as the training set and randomly retrieve N examples from them. Since Spider contains multiple NLQs correspond- ing to the same SQL query, we require that the train- ing set does not contain examples with the same SQL template as the test example, again following previous work (Finegan-Dollak et al., 2018). In the cross-domain scenario, we randomly se- lect M demonstration databases, each with K NLQ-SQL pairs (M x K = N) from the Spider training set. Incorporating multiple demonstration databases in a prompt text significantly increases its length. Hence, we only use Codex for the cross- domain experiments, due to its higher token limit of 8K, surpassing the 4K limit of ChatGPT. In both single-domain and cross-domain settings, we com- pare different prompt construction methods using the same few-shot examples to make a fair compar- ison. We repeat our experiments three times and present the average results. 5 Results\n\nIn this section, we present our empirical findings in the areas of zero-shot, single-domain, and cross- domain text-to-SQL. Through our experiments, we aim to answer a few crucial research questions in each setting and provide insightful strategies for future studies on effective prompting. 5.1 Zero-shot Text-to-SQL\n\nIn the zero-shot setting, we focus on comparing different prompt constructions for databases. Table\n2We employ the Code-davinci-002 version of Codex across all settings. In zero-shot and single-domain setups, we uti- lize the gpt-3.5-turbo-0301 version of ChatGPT. For cross- domain experiments involving ChatGPT-16K, we turned to gpt-3.5-turbo-16k-0613 due to its extended context length. <table><thead><tr><th></th><th></th><th></th><th colspan="2">Codex</th><th colspan="2">ChatGPT</th></tr><tr><th colspan="2">Database Prompt Construction</th><th></th><th># Tokens (UIN)</th><th>EX (UIN)</th><th># Tokens (UIN)</th><th>EX (UIN)</th></tr></thead><tbody><tr><td rowspan="2">Table Schema</td><td>Table(Columns)</td><td></td><td>1481147</td><td>69.0171.9</td><td>1181115</td><td>68.8170.5</td></tr><tr><td>Columns=[]</td><td></td><td>1691167</td><td>70.2171.8</td><td>1371135</td><td>68.3169.1</td></tr><tr><td rowspan="2">+Relationship</td><td>Columns=[]+ForeignKey</td><td></td><td>2261223</td><td>723173.1</td><td>1781174</td><td>729171.2</td></tr><tr><td>CreateTable</td><td></td><td>4741356</td><td>71.8173.1</td><td>3391254</td><td>70.7171.7</td></tr><tr><td rowspan="3">+Relationship+Content</td><td>CreateTable+InsertRow</td><td>3</td><td>108911013</td><td>709171.9</td><td>964 | 872</td><td>7181718</td></tr><tr><td>CreateTable+SelectRow</td><td>3</td><td>8201770</td><td>733174.1</td><td>7611674</td><td>71.8172.1</td></tr><tr><td>CreateTable+SelectCol</td><td>3</td><td>9581831</td><td>75.0175.7</td><td>7991712</td><td>733173.6</td></tr></tbody></table>\n\nTable 1: Zero-shot results of Codex and ChatGPT using different database prompt constructions. Table Schema (upper part) contains prompts that solely include the schema of tables, while +Relationship (middle part) incor- porates foreign keys as the table relationships and +Relationship+Content (lower part) adds table content as well. # Tokens is the average token counts in the prompts and EX represents the execution accuracy of SQLs. UIN represents the results of unnormalized prompts and normalized prompts, respectively. The underlines highlight the lower number of tokens and higher accuracies when comparing unnormalized and normalized prompts and the highest accuracy achieved among all prompts is highlighted in bold. 1 shows the average prompt length and execution accuracy of Codex and ChatGPT using various database prompt constructions. Q1: How does normalized database prompt perform compared to unnormalized ones? Nor- malized schemas are found to have a reduced to- ken count in comparison to unnormalized schemas across all database constructions. The normaliza- tion also tends to yield slightly better performance. As for Codex, normalized schemas show improve- ment in all prompts. For ChatGPT, normalized schemas either improve accuracy or achieve the same accuracy or achieve the same level of accu- racy as unnormalized schemas in 6 out of 7 schema constructions. The tests of statistical significance are presented in Appendix A.2. Q2: What database knowledge is crucial for effectively prompting LLMs? Our experiments indicate that table relationships and content are im- portant. The Columns=[] prompt includes only the table schema, while the Columns=[]J+ForeignKey prompt contains the additional relationship among tables shown as foreign keys. Including such information improves the performance of both Codex (71.8 -> 73.1) and ChatGPT (69.1 -> 71.2). Moveover, exposing LLMs to database content with the SelectRow and SelectCol prompts fur- ther enhances the performance of both Codex and ChatGPT, while the InsertRow prompt does not seem to be beneficial. We believe that database content is valuable, but its representation needs to be carefully chosen. ChatGPT? While we do not focus on comparing different LLMs on the text-to-SQL tasks in this paper, it is worth noting that Codex consistently outperforms ChatGPT on zero-shot text-to-SQL using various prompt constructions. Based on all the findings above, we would recom- mend using Codex in conjunction with normalized CreateTableSelectCol prompt construction for zero-shot text-to-SQL.3\n\n\n5.2 Single-domain Text-to-SQL\n\nIn the zero-shot text-to-SQL setting, we discovered that the prompt constructions of databases impact the performance of LLMs. This discovery naturally raises the question of whether the introduction of in-domain demonstrations affects the performance of LLMs to different database prompts. Q1: Does the use of in-domain demonstrations enhance LLM’s performance? Figure 5 depicts the performance of Codex and ChatGPT using dif- ferent database prompt constructions with respect to different numbers of in-domain demonstration examples. For all database prompts, the perfor- mance of LLMs experiences a notable improve- ment when in-domain examples are presented. Fur- thermore, the performance continues to enhance as the number of in-domain examples increases. Q2: What database knowledge is important when presenting in-domain demonstrations? While we have observed that the presence of table\n\n\nQ3: How does Codex perform compared to\n\n5 R . . “To simplify our experiments and ensure consistent prompts, we adopt normalization for single-domain and cross- domain experiments. ###IMAGE###Figure-5###IMAGE###\nFigure 5: Execution accuracy of Codex and ChatGPT for single-domain text-to-SQL with 1, 4, 8, and 16 in- domain examples. RS and Cont correspond to table relationship and table content, respectively.'), Document(metadata={'pdf_name': 'pdf1', 'datetime': '2024-11-17 15:04:11.574486+07:00'}, page_content='Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: The atis-3 corpus. In Proceedings of the Workshop on Human Language Technology, pages 43—48. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.')]